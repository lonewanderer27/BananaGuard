services:
  backend:
    build: ./backend
    container_name: "banana_api"
    volumes:
      - ./model:/app/model
      - ./backend:/app
    environment:
      - MODEL_PATH=/app/model

  frontend:
    build: ./frontend
    container_name: "banana_web"
    ports:
      - "5174:5174"
    depends_on:
      - backend
      - llm

  llm:
    build: ./llm
    container_name: banana_llm
    environment:
      # Detect at runtime whether Ollama is in Docker or host
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - CHROMA_URL=${CHROMA_URL:-http://chromadb:8002}
    depends_on:
      - ollama
      - chromadb

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ./ollama:/root/.ollama
      - ./init-ollama.sh:/init-ollama.sh
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    volumes:
      - chroma_data:/chroma

volumes:
  ollama_data:
  chroma_data:

networks:
  default:
    name: banana_net
